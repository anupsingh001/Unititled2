{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reJvAXtLfOAp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine (SVM)\n",
        "1: What is a Support Vector Machine (SVM)?\n",
        "SVM is a supervised machine learning algorithm used for classification and regression tasks. It finds the best hyperplane that separates data points of different classes with the maximum margin.\n",
        "\n",
        "2: What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "Hard Margin SVM: Assumes the data is linearly separable with no misclassifications. Strict and sensitive to outliers.\n",
        "\n",
        "Soft Margin SVM: Allows some misclassifications by introducing a penalty parameter (C) to balance margin size and classification error. Better for real-world data.\n",
        "\n",
        "3: What are Support Vectors in SVM?\n",
        "These are the data points closest to the hyperplane. They lie on the margin boundaries and are critical in defining the hyperplane.\n",
        "\n",
        "4: What is a Support Vector Classifier (SVC)?\n",
        "It is the implementation of SVM for classification tasks. Scikit-learn's SVC is a popular example.\n",
        "\n",
        "5 : What is a Support Vector Regressor (SVR)?**\n",
        "SVR is the regression counterpart of SVM. Instead of maximizing the margin, it fits data within an epsilon-insensitive tube and penalizes deviations outside it.\n",
        "\n",
        "6: What is the Kernel Trick in SVM?\n",
        "It allows SVMs to perform in non-linear feature spaces without explicitly computing transformations. It uses kernel functions like:\n",
        "\n",
        "Linear\n",
        "\n",
        "Polynomial\n",
        "\n",
        "Radial Basis Function (RBF)\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "\n",
        "10: What is the effect of the C parameter in SVM?\n",
        "\n",
        "C controls the trade-off between margin width and misclassification.\n",
        "\n",
        "Large C ‚Üí less tolerance for error (hard margin-like).\n",
        "\n",
        "Small C ‚Üí more tolerance, wider margin.\n",
        "\n",
        "11: What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "Gamma defines how far the influence of a single training point reaches.\n",
        "\n",
        "Low gamma ‚Üí far reach ‚Üí smoother boundaries.\n",
        "\n",
        "High gamma ‚Üí close reach ‚Üí complex boundaries (can overfit).\n",
        "\n",
        "Na√Øve Bayes\n",
        "12: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "Na√Øve Bayes classifier is a probabilistic machine learning algorithm based on Bayes‚Äô Theorem, used primarily for classification tasks. It assumes independence between features, which is why it is termed \"na√Øve.\"\n",
        "\n",
        "Classifier: It assigns a class label to a given input based on probability.\n",
        "\n",
        "Why \"Na√Øve\"? It assumes that all features are conditionally independent given the class label ‚Äî which is rarely true in real-world data. However, despite this unrealistic assumption, it often performs surprisingly well in practice.\n",
        "\n",
        "13: What is Bayes‚Äô Theorem?\n",
        "Bayes‚Äô Theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "=\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "‚ãÖ\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ")\n",
        "P(C‚à£X)=\n",
        "P(X)\n",
        "P(X‚à£C)‚ãÖP(C)\n",
        "‚Äã\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "P(C‚à£X): Posterior probability (class given features)\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "P(X‚à£C): Likelihood (features given class)\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        ")\n",
        "P(C): Prior probability of the class\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ")\n",
        "P(X): Evidence (total probability of features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "14: Differences between Gaussian, Multinomial, and Bernoulli Na√Øve Bayes:\n",
        "\n",
        "Variant\tSuitable For\tDistribution Assumption\n",
        "Gaussian\tContinuous data\tNormal distribution\n",
        "Multinomial\tDiscrete data (e.g., word counts)\tMultinomial distribution\n",
        "Bernoulli\tBinary features (e.g., word presence)\tBernoulli distribution (0 or 1)\n",
        "\n",
        "15: When should you use Gaussian Na√Øve Bayes over other variants?\n",
        "Use it when your features are continuous and normally distributed ‚Äî e.g., image pixel values, sensor readings.\n",
        "\n",
        "16: What are the key assumptions made by Na√Øve Bayes?\n",
        "\n",
        "Feature independence: All features contribute independently to the class.\n",
        "\n",
        "Equal importance: Each feature has equal influence (unless weighted differently).\n",
        "\n",
        "17: What are the advantages and disadvantages of Na√Øve Bayes?\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple and fast\n",
        "\n",
        "Works well with high-dimensional data\n",
        "\n",
        "Effective for text classification\n",
        "\n",
        "Handles missing data\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Assumes feature independence\n",
        "\n",
        "Can be outperformed by complex models\n",
        "\n",
        "Poor performance with correlated features\n",
        "\n",
        "18: Why is Na√Øve Bayes a good choice for text classification?\n",
        "\n",
        "Text data is high-dimensional and sparse\n",
        "\n",
        "Word occurrences can be treated as independent features\n",
        "\n",
        "Fast training and prediction\n",
        "\n",
        "Performs well in spam detection, sentiment analysis\n",
        "\n",
        "19: Compare SVM and Na√Øve Bayes for classification tasks:\n",
        "\n",
        "Aspect\tSVM\tNa√Øve Bayes\n",
        "Speed (Training)\tSlower\tVery fast\n",
        "Performance\tHigh (especially with tuning)\tCompetitive in many cases\n",
        "Assumptions\tMargin maximization\tFeature independence\n",
        "Works well for\tComplex boundaries\tText, high-dimensional data\n",
        "Probabilistic Output\tNot inherently\tYes\n",
        "\n",
        "20: How does Laplace Smoothing help in Na√Øve Bayes?\n",
        "In Na√Øve Bayes, when calculating probabilities, we often encounter a situation where a particular feature value does not occur with a certain class in the training data.\n",
        "This leads to:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚à£\n",
        "ùë¶\n",
        ")\n",
        "=\n",
        "0\n",
        "P(x\n",
        "i\n",
        "‚Äã\n",
        " ‚à£y)=0\n",
        "And since Na√Øve Bayes multiplies probabilities, this zero makes the entire posterior probability zero, which is undesirable ‚Äî especially if the event is rare but possible.\n",
        "\n",
        "‚úÖ Solution: Laplace Smoothing (a.k.a. Add-One Smoothing)\n",
        "Laplace Smoothing adjusts probability estimates to avoid zero probabilities by adding a small constant (usually 1) to each count.\n",
        "\n",
        "Formula without smoothing:\n",
        "ùëÉ\n",
        "(\n",
        "ùë§\n",
        "ùëñ\n",
        "‚à£\n",
        "ùë¶\n",
        ")\n",
        "=\n",
        "count\n",
        "(\n",
        "ùë§\n",
        "ùëñ\n",
        ",\n",
        "ùë¶\n",
        ")\n",
        "‚àë\n",
        "ùëó\n",
        "count\n",
        "(\n",
        "ùë§\n",
        "ùëó\n",
        ",\n",
        "ùë¶\n",
        ")\n",
        "P(w\n",
        "i\n",
        "‚Äã\n",
        " ‚à£y)=\n",
        "‚àë\n",
        "j\n",
        "‚Äã\n",
        " count(w\n",
        "j\n",
        "‚Äã\n",
        " ,y)\n",
        "count(w\n",
        "i\n",
        "‚Äã\n",
        " ,y)\n",
        "‚Äã\n",
        "\n",
        "With Laplace Smoothing (add-1):\n",
        "ùëÉ\n",
        "(\n",
        "ùë§\n",
        "ùëñ\n",
        "‚à£\n",
        "ùë¶\n",
        ")\n",
        "=\n",
        "count\n",
        "(\n",
        "ùë§\n",
        "ùëñ\n",
        ",\n",
        "ùë¶\n",
        ")\n",
        "+\n",
        "1\n",
        "‚àë\n",
        "ùëó\n",
        "count\n",
        "(\n",
        "ùë§\n",
        "ùëó\n",
        ",\n",
        "ùë¶\n",
        ")\n",
        "+\n",
        "ùëâ\n",
        "P(w\n",
        "i\n",
        "‚Äã\n",
        " ‚à£y)=\n",
        "‚àë\n",
        "j\n",
        "‚Äã\n",
        " count(w\n",
        "j\n",
        "‚Äã\n",
        " ,y)+V\n",
        "count(w\n",
        "i\n",
        "‚Äã\n",
        " ,y)+1\n",
        "‚Äã\n",
        "\n",
        "Where:\n",
        "\n",
        "ùë§\n",
        "ùëñ\n",
        "w\n",
        "i\n",
        "‚Äã\n",
        " : a word or feature\n",
        "\n",
        "ùë¶\n",
        "y: the class label\n",
        "\n",
        "ùëâ\n",
        "V: total number of unique features (e.g., vocabulary size in text classification)\n",
        "\n",
        "üìå Example (Text Classification):\n",
        "Imagine you're classifying messages as spam or not spam, and you're using word frequencies. If a word like \"lottery\" hasn't appeared in any spam messages during training, you'll get:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "lottery\n",
        "‚à£\n",
        "spam\n",
        ")\n",
        "=\n",
        "0\n",
        "P(lottery‚à£spam)=0\n",
        "Without smoothing, the entire spam probability becomes 0 for any message containing \"lottery\".\n",
        "With Laplace smoothing:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "lottery\n",
        "‚à£\n",
        "spam\n",
        ")\n",
        "=\n",
        "0\n",
        "+\n",
        "1\n",
        "total¬†word¬†count¬†in¬†spam\n",
        "+\n",
        "ùëâ\n",
        "P(lottery‚à£spam)=\n",
        "total¬†word¬†count¬†in¬†spam+V\n",
        "0+1\n",
        "‚Äã\n",
        "\n",
        "Now, the model assigns a small, non-zero probability instead of zero ‚Äî preventing incorrect elimination of a possible class.\n",
        "\n",
        "üü¢ Benefits of Laplace Smoothing:\n",
        "Prevents zero probability errors\n",
        "\n",
        "Enables the model to handle unseen features\n",
        "\n",
        "Makes the model more robust to sparse data\n"
      ],
      "metadata": {
        "id": "gH9QLsU5fPNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Iris Dataset - Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "iB8UNLqPj8rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Kernel\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_pred = linear_svm.predict(X_test)\n",
        "linear_acc = accuracy_score(y_test, linear_pred)\n",
        "\n",
        "# RBF Kernel\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_pred = rbf_svm.predict(X_test)\n",
        "rbf_acc = accuracy_score(y_test, rbf_pred)\n",
        "\n",
        "print(f\"Linear SVM Accuracy: {linear_acc}\")\n",
        "print(f\"RBF SVM Accuracy: {rbf_acc}\")\n"
      ],
      "metadata": {
        "id": "A2h1WJdAkNVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVR\n",
        "model = SVR(kernel='rbf')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"SVR - Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "1fZWiVvmkUnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
        "                           n_clusters_per_class=1, n_samples=100, random_state=42)\n",
        "\n",
        "# Train SVM with polynomial kernel\n",
        "model = SVC(kernel='poly', degree=3)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Plot decision boundary\n",
        "def plot_decision_boundary(clf, X, y):\n",
        "    h = .02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "    plt.title(\"SVM with Polynomial Kernel\")\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(model, X, y)\n"
      ],
      "metadata": {
        "id": "RQFZ4Tdykcmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLtapYzYkkAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian NB\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Breast Cancer Dataset - Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "jkNrIcE4kkR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all')\n",
        "X, y = newsgroups.data, newsgroups.target\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Multinomial NB\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"20 Newsgroups - Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "v0HPBNznkr8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X, y = datasets.make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
        "                                    n_clusters_per_class=1, n_samples=100, random_state=42)\n",
        "\n",
        "C_vals = [0.01, 1, 100]\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, C in enumerate(C_vals):\n",
        "    model = SVC(kernel='linear', C=C)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "    plt.title(f'C = {C}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vq3gwfRpkzpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_classes=2, random_state=42)\n",
        "X_binary = (X > 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_binary, y, test_size=0.2)\n",
        "\n",
        "model = BernoulliNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Bernoulli NB Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "JfjLKg_4lFtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Without scaling\n",
        "model_raw = SVC()\n",
        "model_raw.fit(X_train, y_train)\n",
        "print(\"Without Scaling Accuracy:\", model_raw.score(X_test, y_test))\n",
        "\n",
        "# With scaling\n",
        "model_scaled = make_pipeline(StandardScaler(), SVC())\n",
        "model_scaled.fit(X_train, y_train)\n",
        "print(\"With Scaling Accuracy:\", model_scaled.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "0tiZLvealMRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "pred_before = gnb.predict(X_test)\n",
        "\n",
        "# Simulated Laplace Smoothing by adding small value to variance\n",
        "gnb.var_ += 1e-9\n",
        "pred_after = gnb.predict(X_test)\n",
        "\n",
        "print(\"Accuracy before smoothing:\", accuracy_score(y_test, pred_before))\n",
        "print(\"Accuracy after smoothing:\", accuracy_score(y_test, pred_after))\n"
      ],
      "metadata": {
        "id": "MjQbIv1tlTJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "hxrcwhSIlcUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Create imbalance\n",
        "X_imb, y_imb = X, np.where(y == 0, 0, 1)\n",
        "y_imb[:90] = 0  # majority class\n",
        "\n",
        "model = SVC(class_weight='balanced')\n",
        "model.fit(X_imb, y_imb)\n",
        "print(\"Imbalanced Accuracy with Class Weight:\", model.score(X_imb, y_imb))\n"
      ],
      "metadata": {
        "id": "Ei6SEcNqlkR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "emails = [\"Free offer just for you\", \"Hi friend, let's meet tomorrow\", \"Claim your free money now\", \"How are you doing today\"]\n",
        "labels = [1, 0, 1, 0]  # 1=Spam, 0=Ham\n",
        "\n",
        "vec = CountVectorizer(binary=True)\n",
        "X = vec.fit_transform(emails)\n",
        "\n",
        "model = BernoulliNB()\n",
        "model.fit(X, labels)\n",
        "\n",
        "print(\"Predictions:\", model.predict(X))\n"
      ],
      "metadata": {
        "id": "wS40RehOlsnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "svm = SVC()\n",
        "nb = GaussianNB()\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "print(\"SVM Accuracy:\", svm.score(X_test, y_test))\n",
        "print(\"Na√Øve Bayes Accuracy:\", nb.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "Lyok6454l0CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "selector = SelectKBest(score_func=chi2, k=10)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "X_train_fs, X_test_fs, y_train, y_test = train_test_split(X_new, y, test_size=0.2)\n",
        "\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train_fs, y_train)\n",
        "\n",
        "print(\"Na√Øve Bayes Accuracy after Feature Selection:\", nb.score(X_test_fs, y_test))\n"
      ],
      "metadata": {
        "id": "aR49Rmtul9_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(*load_wine(return_X_y=True), test_size=0.2)\n",
        "\n",
        "model_ovr = OneVsRestClassifier(SVC())\n",
        "model_ovo = OneVsOneClassifier(SVC())\n",
        "\n",
        "model_ovr.fit(X_train, y_train)\n",
        "model_ovo.fit(X_train, y_train)\n",
        "\n",
        "print(\"OvR Accuracy:\", model_ovr.score(X_test, y_test))\n",
        "print(\"OvO Accuracy:\", model_ovo.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "bIi9fmNumCUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(SVC(), X, y, cv=skf)\n",
        "\n",
        "print(\"Stratified K-Fold Accuracy (mean):\", scores.mean())\n"
      ],
      "metadata": {
        "id": "osqc0SlVmHof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "model = SVC(kernel='linear')\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "\n",
        "print(\"Average Accuracy (Stratified K-Fold):\", scores.mean())\n"
      ],
      "metadata": {
        "id": "JgFZ6YfsmNrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "priors = [[0.5, 0.5], [0.7, 0.3], [0.3, 0.7]]\n",
        "for p in priors:\n",
        "    model = GaussianNB(priors=p)\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"Accuracy with priors {p}: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "vnL4UJZnmgzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# RFE with 5 selected features\n",
        "svm = SVC(kernel='linear')\n",
        "selector = RFE(estimator=svm, n_features_to_select=5)\n",
        "selector.fit(X_train, y_train)\n",
        "\n",
        "model = svm.fit(selector.transform(X_train), y_train)\n",
        "y_pred = model.predict(selector.transform(X_test))\n",
        "\n",
        "print(\"Accuracy after RFE:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "etju9SJ5ml25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "id": "HcxnLOcSmq3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "print(\"Log Loss:\", log_loss(y_test, y_proba))\n"
      ],
      "metadata": {
        "id": "sSalSmSsmwUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"SVM Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gai_tOs7m1W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "model = SVR()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Mean Absolute Error (SVR):\", mean_absolute_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Bm9xo9d9m6en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# For binary classification\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# Binarize labels for ROC AUC\n",
        "y_bin = label_binarize(y_test, classes=[0, 1])\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_bin, y_proba[:, 1]))\n"
      ],
      "metadata": {
        "id": "-VGotP4Gm_NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#46\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "model = SVC(probability=True)\n",
        "model.fit(X_train, y_train)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"SVM Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "c8qqZLwdnD5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#47\n"
      ],
      "metadata": {
        "id": "OCS2dIWAnMBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbEeg2F0nFKH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}